{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heesoo-jang/comp664-deep-learning-spring-2023/blob/main/COMP664_Homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Logistic Regression for Binary Classification (1.5 points)\n",
        "\n",
        "In softmax regression, the model's output probabilities are given by\n",
        "\n",
        "$$p(y|\\mathbf{o}) = \\mathrm{softmax}(\\mathbf{o})_y = \\frac{\\exp(o_y)}{\\sum_{y'} \\exp(o_{y'})}$$\n",
        "\n",
        "1. Show that this parametrization has a spurious degree of freedom. That is, show that both $\\mathbf{o}$ and $\\mathbf{o} + c$ with $c \\in \\mathbb{R}$ lead to the same probability estimate.\n",
        "\n",
        "2. For binary classification, i.e. whenever we have only two classes $\\{-1, 1\\}$, we can arbitrarily set $o_{-1} = 0$. Using the shorthand $o = o_1$ show that this is equivalent to \n",
        "\n",
        "$$p(y=1|o) = \\frac{1}{1 + \\exp(-o)}$$\n",
        "\n",
        "3. Show that the log-likelihood loss (often called logistic loss) for labels $y \\in \\{-1, 1\\}$ is thus given by \n",
        "\n",
        "$$-\\log p(y|o) = \\log (1 + \\exp(-y \\cdot o))$$\n",
        "\n",
        "4. Show that for $y = 1$ the logistic loss asymptotes to $0$ for $o \\to \\infty$ and to $\\infty$ for $o \\to -\\infty$. "
      ],
      "metadata": {
        "id": "0VV0ToY_fa2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Implementing the logistic loss (1 point)\n",
        "\n",
        "1. Implement the binary logistic loss $l(y,o) = \\log (1 + \\exp(-y \\cdot o))$ in numpy.\n",
        "1. Plot its values for $y \\in \\{-1, 1\\}$ over the range of $o \\in [-5, 5]$.\n",
        "1. Derive the derivative of $l(y,o)$ and show your work. Then, in a new plot, plot derivative of $l(y,o)$ with respect to $o$ for $y \\in \\{-1, 1\\}$ and for $o \\in [-5, 5]$ without using any automatic differentiation software (e.g. Pytorch, TensorFlow, JAX, etc.).\n",
        "1. In a new plot, plot $l(y,o)$ with $y = 1$ and $o âˆˆ [-1000, 1000]$. What happens for large positive and large negative values of $o$? Why?"
      ],
      "metadata": {
        "id": "dMcRnYVRfxfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implementing logistic regression (1.5 points)\n",
        "\n",
        "Implement a basic logistic regression model and fit it to the data from the new `generate_data` function (below) using gradient descent.\n",
        "Your model should take the form `o = m*x + b`, where `o` is the output (logits), `x` is the input, `m` is a weight parameter, and `b` is a bias parameter.\n",
        "Train the model using the logistic loss function you derived in the previous problem.\n",
        "You must use only `numpy` and derive any derivatives yourself (i.e. no autograd from TensorFlow, MXNet, Pytorch, JAX etc!).\n",
        "Print out or plot the loss over the course of training.\n",
        "You should be able to get a loss that is close to zero by the end of training - make sure you achieve this and **explain why**."
      ],
      "metadata": {
        "id": "pSBRk8vpsqA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_data():\n",
        "    size = 1000\n",
        "    rng = np.random.default_rng(seed=0)\n",
        "    y = 2*rng.integers(0, 2, size) - 1\n",
        "    x = rng.standard_normal((1000,)) + 4*y\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "L150buNSoFes"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}